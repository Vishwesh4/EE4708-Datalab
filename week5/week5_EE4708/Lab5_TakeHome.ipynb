{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MMWp3bQP-bhU"
   },
   "source": [
    "# Lab 5 - Classification : Naive Bayes and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n-5YUdzFsqwL"
   },
   "source": [
    "# Optical recognition of handwritten digits dataset\n",
    "**Download dataset from sklearn. The dataset has 10 classes and 64 attributes (8x8 images). Visualise images from the dataset. Perform a train test split in the ratio 4:1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB,BernoulliNB,MultinomialNB,ComplementNB\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "X = np.resize(digits.images,(len(digits.images),64))\n",
    "Y = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ffa42398690>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.gray()\n",
    "plt.title(\"Image of {}\".format(digits.target[5]))\n",
    "plt.imshow(digits.images[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6w_FSrzBE5pw"
   },
   "source": [
    "# Using sklearn (25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ANyyLqRaE-RA"
   },
   "source": [
    "**For this exercise, you will use the naive bayes and logistic regression functions in sklearn. Use the optical recognition dataset.**\n",
    "\n",
    "\n",
    "**a) Logistic Regression - use one vs all classification method to classify the dataset into one of the ten classes. Report the accuracies in terms of F1 scores and the confusion matrix (use sklearn functions for this too). Tune parameters to obtain the best results.**\n",
    "\n",
    "**b) Naive Bayes - perform multiclass classification to classify the dataset into one of the ten classes. Experiment with all the priors available (Gaussian, Bernoulli, etc) and report the best prior. Report the accuracies in terms of F1 scores and the confusion matrix (use sklearn functions for this too).**\n",
    "\n",
    "**Estimated Time: 50 mins**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.9669777560237893\n",
      "Confusion matrix:[[38  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 35  0  0  0  0  0  0  2  0]\n",
      " [ 0  0 39  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 38  0  1  0  0  2  0]\n",
      " [ 0  0  0  0 40  0  0  1  0  0]\n",
      " [ 0  0  0  0  0 27  0  0  0  0]\n",
      " [ 0  0  0  0  0  1 29  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 35  0  1]\n",
      " [ 0  2  0  0  0  0  0  0 32  0]\n",
      " [ 0  0  0  0  0  0  0  1  1 35]]\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:   11.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model->LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l1',\n",
      "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Best F1 score: 0.9750992881234128\n",
      "Best Confusion matrix:[[38  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 37  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 39  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 38  0  2  0  0  1  0]\n",
      " [ 0  0  0  0 40  0  0  1  0  0]\n",
      " [ 0  0  0  0  0 27  0  0  0  0]\n",
      " [ 0  0  0  0  0  1 29  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 35  0  1]\n",
      " [ 0  1  0  0  0  0  0  0 33  0]\n",
      " [ 0  0  0  0  0  0  0  1  1 35]]\n"
     ]
    }
   ],
   "source": [
    "#Part A Logistic Regression\n",
    "#default is one vs all\n",
    "#Default model\n",
    "Log_model = LogisticRegression().fit(X_train,y_train)\n",
    "#testing accuracies\n",
    "y_pred = Log_model.predict(X_test)\n",
    "#f1 score\n",
    "f1 = f1_score(y_test,y_pred,average=\"weighted\")\n",
    "print(\"F1 score: {}\".format(f1))\n",
    "#Confusion matrix\n",
    "con_mat = confusion_matrix(y_test,y_pred)\n",
    "print(\"Confusion matrix:{}\".format(con_mat))\n",
    "\n",
    "#Grid search cv for best hyper parmaeters\n",
    "parameters = {'penalty':('l1', 'l2'), 'C':[0.001,0.01,0.1,1, 10]}\n",
    "best_model = LogisticRegression()\n",
    "clf = GridSearchCV(best_model, parameters, cv=5,verbose=1)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "#f1 score\n",
    "f1 = f1_score(y_test,y_pred,average=\"weighted\")\n",
    "print(\"Best model->{}\".format(clf.best_estimator_))\n",
    "print(\"Best F1 score: {}\".format(f1))\n",
    "#Confusion matrix\n",
    "con_mat = confusion_matrix(y_test,y_pred)\n",
    "print(\"Best Confusion matrix:{}\".format(con_mat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EvL8quVdN24q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "0.8572210348819872\n",
      "Confusion matrix for GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "[[36  0  0  0  1  0  0  1  0  0]\n",
      " [ 0 31  0  0  0  0  0  1  3  2]\n",
      " [ 0  1 34  0  0  0  0  0  4  0]\n",
      " [ 0  1  0 33  0  2  0  2  3  0]\n",
      " [ 0  0  0  0 36  0  0  5  0  0]\n",
      " [ 0  0  0  0  0 27  0  0  0  0]\n",
      " [ 0  0  1  0  1  0 28  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 36  0  0]\n",
      " [ 0  3  0  1  0  1  0  4 25  0]\n",
      " [ 1  2  0  0  1  0  0  8  3 22]]\n",
      "F1 score for BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "0.8690023526777814\n",
      "Confusion matrix for BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "[[36  0  0  0  2  0  0  0  0  0]\n",
      " [ 0 26  4  0  0  1  0  0  3  3]\n",
      " [ 0  1 36  1  0  0  0  0  1  0]\n",
      " [ 0  0  2 34  0  1  0  1  2  1]\n",
      " [ 0  0  0  0 39  0  0  2  0  0]\n",
      " [ 0  0  0  0  0 22  0  0  0  5]\n",
      " [ 0  0  0  0  1  1 28  0  0  0]\n",
      " [ 0  0  0  0  1  0  0 35  0  0]\n",
      " [ 0  2  1  1  0  0  1  1 27  1]\n",
      " [ 0  2  0  0  0  0  0  4  1 30]]\n",
      "F1 score for MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "0.8895164605564936\n",
      "Confusion matrix for MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "[[37  0  0  0  1  0  0  0  0  0]\n",
      " [ 0 29  2  0  0  0  0  0  3  3]\n",
      " [ 0  2 37  0  0  0  0  0  0  0]\n",
      " [ 0  0  1 35  0  1  0  2  2  0]\n",
      " [ 0  0  0  0 38  0  0  3  0  0]\n",
      " [ 0  0  0  0  0 23  0  0  0  4]\n",
      " [ 0  0  0  0  1  0 29  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 36  0  0]\n",
      " [ 0  4  0  0  0  0  1  1 27  1]\n",
      " [ 0  1  0  0  0  0  0  4  3 29]]\n",
      "F1 score for ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)\n",
      "0.7913758928232614\n",
      "Confusion matrix for ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)\n",
      "[[37  0  0  0  1  0  0  0  0  0]\n",
      " [ 0 28  5  0  1  0  0  0  0  3]\n",
      " [ 0  1 38  0  0  0  0  0  0  0]\n",
      " [ 0  0  5 31  0  1  1  3  0  0]\n",
      " [ 0  0  0  0 37  0  0  4  0  0]\n",
      " [ 0  0  0  0  0 26  0  1  0  0]\n",
      " [ 1  0  0  0  1  0 28  0  0  0]\n",
      " [ 0  0  1  0  2  0  0 33  0  0]\n",
      " [ 0  3  2  2  1  2  6  9  8  1]\n",
      " [ 1  1  0  0  0  1  1  9  0 24]]\n"
     ]
    }
   ],
   "source": [
    "#Part B Naive Bayes\n",
    "prior_list = [GaussianNB(),BernoulliNB(),MultinomialNB(),ComplementNB()]\n",
    "\n",
    "for prior in prior_list:\n",
    "    #Prior\n",
    "    model = prior.fit(X_train,y_train)\n",
    "    #testing accuracies\n",
    "    y_pred = model.predict(X_test)\n",
    "    #f1 score\n",
    "    f1 = f1_score(y_test,y_pred,average=\"weighted\")\n",
    "    print(\"F1 score for {}\\n{}\".format(str(prior),f1))\n",
    "    #Confusion matrix\n",
    "    con_mat = confusion_matrix(y_test,y_pred)\n",
    "    print(\"Confusion matrix for {}\\n{}\".format(str(prior),con_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oli7OP9XN4ZF"
   },
   "source": [
    "Based on the F1 scores and confusion matrix we can say that MultinomialNB is the best prior amongst all other priors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Aiq6KaGLOAba"
   },
   "source": [
    "# The End"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab5_DataAnalytics.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
